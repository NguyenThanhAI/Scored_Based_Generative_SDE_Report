\documentclass{article} % For LaTeX2e
\usepackage{iclr2021_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{mathtools}
\usepackage{graphicx}
%\usepackage{algpseudocode}
%\usepackage{algorithm}
\usepackage[ruled,vlined,linesnumbered,algosection]{algorithm2e}
\usepackage{blindtext}
\usepackage{setspace}
\usepackage[utf8]{inputenc}
\usepackage[utf8]{vietnam}
\usepackage[center]{caption}
\usepackage[shortlabels]{enumitem}
\usepackage{fancyhdr} % header, footer
\usepackage{hyperref} % loại bỏ border với mục lục và công thức
\usepackage{url}
\usepackage[nonumberlist, nopostdot, nogroupskip]{glossaries}
\usepackage{glossary-superragged}
\usepackage{tikz,tkz-tab}
%\usepackage[style=numeric,sortcites]{biblatex}
%\addbibresource{ref.bib}
%\usepackage[numbers]{natbib}
\usepackage{indentfirst}
\usepackage{multirow}
\usepackage{cancel}


\graphicspath{{./figures/}}

\title{Mô hình sinh dựa trên điểm số bằng \\ phương trình vi phân ngẫu nhiên}


\author{Nguyễn Chí Thanh \\
Khoa Toán - Cơ - Tin học\\
Trường Đại học Khoa học Tự nhiên\\
Đại học Quốc Gia Hà Nội \\
\texttt{nguyenchithanh\_sdh21@hus.edu.vn} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}
\maketitle

\begin{abstract}
    Tạo nhiễu từ dữ liệu là một việc đơn giản, nhưng tạo dữ liệu từ nhiễu được gọi là mô hình sinh.
    Ở đề tài này, ta sẽ trình bày phương trình vi phân ngẫu nhiên (SDE) biến đổi một phân phối dữ liệu phức tạp thành một phân phối tiên nghiệm biết trước trơn và thông suốt bằng cách dần dần thêm nhiễu vào dữ liệu,
    và một phương trình vi phân ngẫu nhiên (SDE) ngược (theo thời gian) biến đổi từ phân phối tiên nghiệm trở lại phân phối dữ liệu một cách từ từ bằng việc loại bỏ dần nhiễu.
    Một điều quan trọng là phương trình vi phân ngược theo thời gian chỉ phụ thuộc vào trường gradient phụ thuộc vào thời gian (được gọi là điểm số) của phân phối dữ liệu bị xáo trộn bởi nhiễu.
    Bằng việc tận dụng những bước tiến trong mô hình sinh dựa trên điểm, ta có thể ước lượng chính xác điểm với mạng neuron, và sử dụng các bộ giải phương trình vi phân ngẫu nhiên (SDE) bằng phương pháp số để sinh ra các mẫu dữ liệu.
    Ta sẽ chỉ ra rằng khung làm việc này bao gồm các cách tiếp cận trước đây về mô hình sinh dựa trên điểm số và các mô hình khuếch tán xác suất, cho phép các thủ tục mới để sinh dữ liệu ra đời với những khả năng mạnh hơn.
    Đặc biệt, ta sẽ giới thiệu một bộ dự đoán - căn chỉnh để căn chỉnh sai lệch trong quá trình biến đổi của phương trình vi phân ngẫu nhiên ngược thời gian được rời rạc hóa.
    Ta cũng thu được một mạng neuron ODE tương đương được lấy mẫu từ cùng phân phối như SDE, nhưng ngoài ra cũng cho phép tính toán chính xác độ hợp lý, và cải thiện độ hiệu quả của quá trình lấy mẫu.
    Ngoài ra, ta cũng cung cấp một cách mới để giải các bài toán ngược với các mô hình dựa trên điểm số, được giải thích với các thí nghiệm trên sinh mẫu dựa trên điều kiện nhãn, vẽ ảnh và tô màu.
    Kết hợp với nhiều bước tiến về các kiến trúc mô hình, ta đã đạt kỷ lục với việc sinh ảnh trên tập CIFAR-10 với điểm Inception score là 9.89 và FID là 2.20 và độ hợp lý rất tốt 2.99 bits/dim, và thể hiện khả năng sinh ảnh chân thực với ảnh có độ phân giải 1024 $\times$ 1024 lần đầu tiên từ một mô hình sinh dựa trên điểm số.
\end{abstract}

\section{GIỚI THIỆU}

Hai lớp thành công của các mô hình sinh xác suất liên quan đến việc làm biến đổi dữ liệu một cách tuần tự với độ nhiễu tăng dần, sau đó học cách đảo ngược sự biến đổi để tạ thành một mô hình sinh của dữ liệu.
\textit{Score matching with Langevin dynamics} (SMLD) \citep{song2019generative} ước lượng điểm số (ví dụ gradient của log của hàm mật độ xác suất của dữ liệu) tại từng mức nhiễu, sau đó sử dụng động học Langevin để lấy mẫu từ mỗi dãy các mức nhiễu giảm dần trong suốt quá trình sinh dữ liệu.
\textit{Denoising diffusion probabilistic modeling} (DDPM) \citep{sohl2015deep,ho2020denoising} huấn luyện một dãy các mô hình xác suất để đảo ngược từng bước của quá trình bị xáo trộn bởi nhiễu,
sử dụng những kiến thức về dạng chức năng của các phân phối ngược làm cho quá trình huấn luyện dễ dàng hơn.
Trong không gian trạng thái liên tục, DDPM huấn luyện hàm mục tiêu ngầm tính điểm số tại từng mức nhiễu.
Vì vậy, ta gọi hai lớp mô hình này là các \textit{mô hình sinh dựa trên điểm số}.

Các mô hình sinh dựa trên điểm số và các kỹ thuật liên quan \citep{bordes2017learning, goyal2017variational,du2019implicit} đã được chứng minh là hiệu quả trong bài toán sinh ảnh \citep{song2019generative,song2020sliced,ho2020denoising}, âm thanh \citep{chen2020wavegrad,kong2020diffwave}, đồ thị \citep{niu2020permutation} và các khối hình \citep{cai2020learning}.
Để cho phép các phương pháp lấy mẫu mới mở rộng khả năng của các mô hình sinh dựa trên điểm số, ta đề xuất một khung làm việc thống nhất tổng quát hóa các cách tiếp cận trước đó thông qua góc nhìn của phương trình vi phân ngẫu nhiên.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{1.jpg}
    \caption{\textbf{Giải một phương trình SDE đảo ngược thời gian thu được một mô hình sinh dựa trên điểm.}
    Biến đổi dữ liệu thành một phân phối nhiễu đơn giản có thể được thực hiện với một phương trình SDE liên tục theo thời gian.
    Phương trình SDE này có thể được đảo ngược nếu ta biết điểm của phân phối tại từng bước thời gian trung gian, $\nabla_{\bold{x}}\log p_t (\bold{x})$.}
    \label{fig:1}
\end{figure}

Cụ thể, thay vì làm nhiễu với một số hữu hạn của các phân phối nhiễu, ta xét một phân phối biến đổi một cách liên tục theo thời gian tương ứng với quá trình khuếch tán.
Quá trình này khuếch tán dần dần các điểm dữ liệu thành nhiễu ngẫu nhiên và được cho bởi một SDE cho trước không phụ thuộc vào dữ liệu và không có các tham số có thể huấn luyện.
Bằng cách đảo ngược quá trình này, ta có thể biến đổi nhiễu thành dữ liệu một cách trơn cho bài toán sinh mẫu dữ liệu.
Điều quan trọng là quá trình ngược này thỏa mãn SDE ngược theo thời gian \citep{anderson1982reverse}, có thể suy ra từ phương trình SDE thuận được cho bởi điểm số của hàm mật độ xác suất cận biên như là một hàm của thời gian.
Vì vậy ta có thể xấp xỉ SDE ngược thời gian bằng cách huấn luyện một mạng neuron phụ thuộc thời gian để ước lượng điểm số và sau đó tạo ra các mẫu dữ liệu sử dụng các bộ giải SDE bằng phương pháp số.
Ý tưởng chính được trình bày ở hình \ref{fig:1}.

Khung làm việc đề xuất có nhiều đóng góp về mặt lý thuyết và thực tiễn:

\textbf{Lấy mẫu linh hoạt và tính toán độ hợp lý:} Ta có thể sử dụng bất kỳ một bộ giải phương trình SDE đa chức năng nào để tích hợp phương trình SDE đảo ngược thời gian cho bài toán lấy mẫu.
Hơn nữa, ta đề xuất hai phương pháp đặc biệt mà không phù hợp với phương trình SDE nói chung: (i) Bộ lấy mẫu Dự đoán - Căn chỉnh (PC) kết hợp với bộ giải SDE bằng phương pháp số và cách tiếp cận MCMC (Markov Chain Monte Carlo) dựa trên điểm số, như Langevin MCMC \citep{parisi1981correlation} và HMC \citep{neal2011mcmc} và (ii) các bộ lấy mẫu tất định dựa trên dòng xác suất của các phương trình vi phân thường (ODE).
Phương pháp thứ nhất hợp nhất vả cải thiện các phương pháp lấy mẫu hiện có cho các mô hình dựa trên điểm số.
Phương pháp thứ hai cho phép lấy mẫu thích ứng nhanh thông qua một hộp đen là một bộ giải phương trình ODE, các thao tác trên dữ liệu linh hoạt thông qua mã ẩn, mỗi mã có tính duy nhất và điều đáng chú ý ta có thể tính chính xác độ hợp lý.

\textbf{Sinh dữ liệu có điều kiện:} Ta có thể điều chỉnh quá trình sinh dữ liệu bằng cách điều chỉnh các thông tin không có sẵn trong quá trình huấn luyện, bởi vì phương trình SDE đảo ngược thời gian có điều kiện có thể được ước lượng hiệu quả từ các điểm số \textit{không có điều kiện}.
Điều này cho phép các ứng dụng như sinh ảnh có điều kiện về lớp, vẽ ảnh, tô màu cũng như các bài toán ngược khác, tất cả các bài toán trên đều có thể được giải quyết bằng cách sử dụng một mô hình dựa trên điểm số không có điều kiện mà không cần phải huấn luyện lại mô hình.

\textbf{Khung làm việc thống nhất:} Khung làm việc của ta cung cấp một cách thống nhất để khai phá của chỉnh định nhiều phương trình SDE cho việc cải thiện các mô hình sinh dựa trên điểm số.
Các phương pháp của SMLD và DDPM có thể được đưa vào khung làm việc của ta bằng cách rời rạc hóa hai phương trình SDE riêng biệt.
Mặc dù DDPM \citep{ho2020denoising} gần đây đã được báo cáo là có thể đạt được mẫu có chất lượng cao hơn SMLD \citep{song2019generative,song2020improved}, ta chỉ ra điều đó với kiến trúc mạng tốt hơn và các thuật toán lấy mẫu mới cho phép bởi khung làm việc của ta, mô hình sau đó có thể bắt kịp và đạt được đến điểm Inception kỷ lục (9.89) và FID (2.20) trên tập CIFAR-10 cũng như độ chân thực của ảnh có độ phân giải 1024 $\times$ 1024 lần đầu tiên từ một mô hình sinh dựa trên điểm.
Ngoài ra, ta đề xuất một phương trình SDE mới trong khung làm việc của ta thu được độ hợp lý có giá trị 2.99 bits/dim trên tập CIFAR-10, tạo nên một kỷ lục mới cho bài toán này.

\section{Kiến thức nền tảng}

\subsection{Khử nhiều khớp điểm số với động học Langevin (SMLD)}

Đặt $p_{\sigma}(\tilde{\bold{x}} \vert \bold{x}) \vcentcolon = \mathcal{N}(\tilde{\bold{x}};\bold{x}, \sigma^2 \bold{I})$ là một nhân tạo ra nhiễu, và $p_{\sigma}(\tilde{\bold{x}})\vcentcolon = \int p_{\mathrm{data}}(\bold{x}) p_{\sigma}(\tilde{\bold{x}} \vert \bold{x}) d \bold{x}$, với $p_{\mathrm{data}}$ ký hiệu là phân phối dữ liệu.
Ta xét một dãy các mức nhiễu dương $\sigma_{\min} < \sigma_2 < \dots < \sigma_N = \sigma_{\max}$.
Thông thường, $\sigma_{\min}$ đủ nhỏ dể $p_{\sigma_{\min}} \approx p_{\mathrm{data}}(\bold{x})$, và $\sigma_{\max}$ đủ lớn để $p_{\sigma_{\max}}\approx \mathcal{N}(\bold{x}; \bold{0}, \sigma_{\max}^2 \bold{I})$.
\citep{song2019generative} đề xuất huấn luyện một mạng điểm số có điều kiện nhiễu (Noise Conditional Score Network - NCSN), ký hiệu $\bold{s}_{\boldsymbol{\theta}}(\bold{x}, \sigma)$ với một tổng có trọng số của các hàm khớp điểm số khử nhiễu \citep{vincent2011connection}:

\begin{equation} \label{eq:1}
    \boldsymbol{\theta}^{\ast} = \argmin_{\boldsymbol{\theta}} \sum_{i=1}^N \sigma_i^2 \mathbb{E}_{p_{\mathrm{data}}(\bold{x})} \mathbb{E}_{p_{\sigma_i}(\tilde{\bold{x}} \vert \bold{x})} \big\lbrack \lVert \bold{s}_{\boldsymbol{\theta} (\tilde{\bold{x}}, \sigma_i)} - \nabla_{\tilde{\bold{x}}} \log p_{\sigma_i} (\tilde{\bold{x}} \vert \bold{x})  \rVert_2^2 \big\rbrack
\end{equation}

Khi được cho một dữ liệu và một mô hình có kích thước phù hợp, mô hình dựa trên điểm số tối ưu $\bold{s}_{\boldsymbol{\theta}^{\ast}}(\bold{x}, \sigma)$ khớp với $\nabla_{\bold{x}} \log p_{\sigma}(\bold{x})$ tại hầu hết vị trí với $\sigma \in \lbrace \sigma_i \rbrace_{i=1}^N$.
Để lấy mẫu, \citep{song2019generative} chạy M bước của Langevin MCMC để thu được mẫu dữ liệu cho lần lượt từng $p_{\sigma_i}(\bold{x})$:

\begin{equation}
    \bold{x}_i^m = \bold{x}_i^{m-1} + \epsilon_i \bold{s}_{\boldsymbol{\theta}^{\ast}} (\bold{x}_i^{m-1}, \sigma_i) + \sqrt{2 \epsilon_i} \bold{z}_i^m, m = 1, 2, \dots, M
\end{equation}

với $\epsilon_i > 0$ là độ dài bước và $\bold{z}_i^m$ là một biến tuân theo phân phối chuẩn tắc.
Công thức trên được lặp lại cho $i=N, N-1, \dots, 1$ với $\bold{x}_N^0 \sim \mathcal{N}(\bold{x} \vert \bold{0}, \sigma_{\max}^2 \bold{I})$ và $\bold{x}_i^0 = \bold{x}_{i+1}^M$ khi $i < N$. Khi $M \rightarrow \infty$ và $\epsilon_i \rightarrow 0$ với mọi $i, \bold{x}_1^M$ trở thành một mẫu chính xác từ $p_{\sigma_{\min}}(\bold{x})\approx p_{\mathrm{data}}(\bold{x})$ trong một số điều kiện.


\subsection{Mô hình xác suất khuếch tán khử nhiễu (DDPM)}

\citep{sohl2015deep,ho2020denoising} xét một dãy các ngưỡng nhiễu dương $0 < \beta_1, \beta_2, \dots, \beta_N < 1$.
Với từng điểm dữ liệu huấn luyện $\bold{x}_0 \sim p_{\mathrm{data}}(\bold{x})$, một xích Markov rời rạc $\lbrace \bold{x}_1, \bold{x}_2, \dots, \bold{x}_N \rbrace$ được tạo ra sao cho $p(\bold{x}_i \vert \bold{x}_{i-1}) = \mathcal{N}(\bold{x}_i; \sqrt{1-\beta_i} \bold{x}_{i-1}, \beta_i \bold{I})$ và vì vậy $p_{\alpha_i}=\mathcal{N}(\bold{x}_i; \sqrt{\alpha_i}\bold{x}_0, (1-\alpha_i)\bold{I})$,
với $\alpha_i \vcentcolon = \prod_{j=1}^i (1 - \beta_j)$.
Tương tự với SMLD, ta có thể ký hiệu phân phối dữ liệu bị xáo trộn là $p_{\alpha_i}\vcentcolon =  \int p_{\mathrm{data}}(\bold{x}) p_{\alpha_i}(\tilde{\bold{x}} \vert \bold{x}) d \bold{x}$.
Các mức nhiễu được chọn sao cho $\bold{x}_N$ có phân phối xấp xỉ phân phối chuẩn tắc $\mathcal{N}(\bold{0}, \bold{I})$.
Một xích Markov biến phân theo chiều ngược được tham số hóa với $p_{\boldsymbol{\theta}} (\bold{x}_{i-1} \vert \bold{x}_i)=\mathcal{N}\Big(\bold{x}_{i-1}; \dfrac{1}{\sqrt{1-\beta_i}}(\bold{x}_i + \beta_i \bold{s}_{\boldsymbol{\theta}}(\bold{x}_i, i)), \beta_i \bold{I}\Big)$ và được huấn luyện với một biến thể được đánh lại trọng số của cận dưới của độ hợp lý (ELBO):

\begin{equation} \label{eq:3}
    \boldsymbol{\theta}^{\ast} = \argmin_{\boldsymbol{\theta}} \sum_{i=1}^N (1 - \alpha_i) \mathbb{E}_{p_{\mathrm{data}}(\bold{x})} \mathbb{E}_{p_{\alpha_i (\tilde{\bold{x}} vert \bold{x})}} \big \lbrack \lVert \bold{s}_{\boldsymbol{\theta}} (\tilde{\bold{x}}, i) - \nabla_{\tilde{\bold{x}}} \log p_{\alpha_i} (\tilde{\bold{x}} \vert \bold{x}) \rVert_2^2 \big \rbrack
\end{equation}

Sau khi giải phương trình \ref{eq:3} để thu được mô hình tối ưu $\bold{s}_{\boldsymbol{\theta}^{\ast}}(\bold{x}, i)$, các mẫu có thể được tạo ra bằng cách bắt đầu từ một nhiễu tuân theo phân phối chuẩn tắc $\bold{x}_N \sim \mathcal{N}(\bold{0}, \bold{I})$ và sử dụng công thức ước lượng xích Markov ngược sau đây:

\begin{equation} \label{eq:4}
    \bold{x}_{i-1} = \dfrac{1}{\sqrt{1-\beta_i}} \big( \bold{x}_i + \beta_i \bold{s}_{\boldsymbol{\theta}^{\ast}} (\bold{x}_i, i) \big) + \sqrt{\beta_i} \bold{z}_i, i = N, N-1, \dots, 1
\end{equation}

Ta gọi phương pháp này là \textit{ancestral sampling}, khi mà phương pháp này thực hiện lấy mẫu từ một mô hình đồ thị $\prod_{i=1}^N p_{\boldsymbol{\theta}} (\bold{x}_{i-1} \vert \bold{x}_i)$.
Hàm mục tiêu trong công thức \ref{eq:3} là $L_{\mathrm{simple}}$ trong \citep{ho2020denoising} được viết dưới dạng khá tương đồng với công thức \ref{eq:1}.
Cũng như công thức \ref{eq:1}, công thức \ref{eq:3} cũng là tổng có trọng số của các hàm mục tiêu khớp điểm số khử nhiễu, $\nabla_{\bold{x}}\log p_{\alpha_i}(\bold{x})$.
Đáng chú ý là trọng số thứ $i$ được đề cập trong công thức \ref{eq:1} và công thức \ref{eq:3} là $\sigma_i^2$ và $(1-\alpha_i)$ liên quan đến các nhân gây xáo trộn tương ứng trong cùng một dạng chức năng:
$\sigma_i^2 \propto 1/\mathbb{E} \big \lbrack \lVert \nabla_{\bold{x}} \log p_{\sigma_i} (\tilde{\bold{x}} \vert \bold{x}) \rVert_2^2 \big \rbrack$ và $(1-\alpha_i) \propto 1/\mathbb{E} \big \lbrack \lVert \nabla_\bold{x} \log p_{\alpha_i}  (\tilde{\bold{x}} \vert \bold{x})\rVert_2^2 \big \rbrack$

\section{Mô hình sinh dựa trên điểm số với SDE}

Để làm nhiễu hay làm xáo trộn dữ liệu với nhiều mức nhiễu là một chìa khóa dẫn đến thành công của các phương pháp trước đây.
Ta đề xuất tổng quát hóa ý tưởng này hơn nữa trở thành một số vô hạn các mức nhiễu, chẳng hạn phân phối của dữ liệu bị làm nhiễu biến đổi tuân theo một phương trình SDE khi nhiễu tăng dần lên.
Tổng quát của khung làm việc của ta được miêu tả trong hình \ref{fig:2}


\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{2.png}
    \caption{\textbf{Tổng quan của một mô hình sinh dựa trên điểm số với phương trình SDE}
    Ta có thể ánh xạ dữ liệu thành một phân phối nhiễu (phân phối tiên nghiệm) với một phương trình SDE (mục \ref{Pertubring-data-with-SDE}) và đảo ngược phương trình SDE này cho quá trình sinh dữ liệu (mục ).
    Ta cũng có thể đảo ngược dòng xác suất tương ứng phương trình ODE (mục ) thu được một quá trình tất định lấy mẫu từ cùng phân phối với SDE.
    Cả phương trình SDE đảo ngược thời gian và dòng xác suất phương trình ODE có thể đạt được bằng các ước lượng điểm số $\nabla_{\bold{x}} \log p_t (\bold{x})$ (mục \ref{Estimating-Scores-For-The-SDE}).}
    \label{fig:2}
\end{figure}

\subsection{Làm xáo trộn dữ liệu với phương trình SDE} \label{Pertubring-data-with-SDE}

Mục đích của ta là xây dựng một quá trình khuếch tán $\lbrace \bold{x}(t) \rbrace_{t=0}^T$ được đánh chỉ số bởi một biến thời gian liên tục $t \in \lbrack 0, T \rbrack$ ví dụ $\bold{x}(0) \sim p_0$, giả sử nếu ta có một bộ dữ liệu mà các điểm dữ liệu là độc lập và cùng phân phối, và $\bold{x}(T) \sim p_T$ là một dạng rất dễ xử lý để sinh các mẫu dữ liệu hiệu quả.
Hay nói cách khác, $p_0$ là phân phối dữ liệu và $p_T$ là phân phối tiên nghiệm.
Quá trình khuếch tán có thể được mô hình hóa là nghiệm của phương trình Ito SDE:

\begin{equation} \label{eq:5}
    d \bold{x} = \bold{f} (\bold{x}, t) dt + g(t) d \bold{w}
\end{equation}

với $\tilde{\bold{w}}$ là một quá trình Wiener chuẩn tắc (hay còn được gọi là chuyển Brown), $\bold{f}(.,t): \mathbb{R}^d \rightarrow \mathbb{R}^d$ là một hàm vector được gọi là hệ số độ trôi của $\bold{x}(t)$ và $g(.): \mathbb{R} \rightarrow \mathbb{R}$ là một hàm vô hướng được gọi là hệ số khuếch tán của $\bold{x}(t)$.
Để cho thuận tiện cho việc trình bày ta giả sử hệ số khuếch tán là một số vô hướng (thay vì là một ma trận $d \times d$) và không phụ thuộc vào $\bold{x}$, nhưng lý thuyết của ta có thể được tổng quát hóa và vẫn đúng trong trường hợp này (phụ lục ).
Phương trình SDE có một nghiệm riêng mạnh chừng nào các hệ số vẫn là Lipschitz toàn cục trong cả trạng thái và thời gian \citep{oksendal2003stochastic}.
Từ sau đây ta ký hiệu $p_t (\bold{x})$ là hàm mật độ xác suất của $\bold{x}(t)$ và sử dụng $p_{st}(\bold{x}(t) \vert \bold{x}(s))$ để ký hiệu nhân chuyển tiếp từ $\bold{x}(s)$ sang $\bold{x}(t)$, với $0 \leq s < s \leq T$.

Thông thường, $p_T$ là một phân phối phi cấu trúc không bao gồm thông tin nào của $p_0$, ví dụ như phân phối Gaussian với trung bình và phương sai cố định.
Có nhiều cách để thiết kế SDE trong phương trình \ref{eq:5} trong quá trình khuếch tán phân phối dữ liệu thành một phân phối tiên nghiệm cố định.
Ta cung cấp một số ví dụ trong mục  thu được quá trình tổng quát hóa liên tục của SMLD và DDPM.

\subsection{Sinh mẫu dữ liệu bằng cách đảo ngược phương trình SDE}

Ta bắt đầu từ các mẫu dữ liệu nhiễu $\bold{x}(T) \sim p_T$ và đảo ngược quá trình, ta có thể thu được các mẫu $\bold{x}(0) \sim p_0$.
Một kết quả đáng chú ý từ \citep{anderson1982reverse} chỉ ra rằng đảo ngược một quá trình khuếch tán cũng là một quá trình khuếch tán, đi ngược theo thời gian bởi một phương trình SDE đảo ngược thời gian:

\begin{equation} \label{eq:6}
    d \bold{x} = \big \lbrack \bold{f}(\bold{x}, t) - g(t)^2 \nabla_{\bold{x}} \log p_t (\bold{x}) \big \rbrack dt + g(t) d \hat{\bold{w}}
\end{equation}

với $\hat{\bold{w}}$ là một quá trình Wiener chuẩn tắc khi thời gian đi ngược từ $T$ về $0$, và $dt$ là một vi phân gia số thời gian âm.
Một khi điểm số của phân phối cận biên $\nabla_{\bold{x}} p_t(\bold{x})$ được biết với mọi $t$, ta có thể thu được quá trình khuếch tán ngược từ phương trình \ref{eq:6} và mô phỏng quá trình này để thu được mẫu dữ liệu từ phân phối $p_0$.

\subsection{Ước lượng điểm số cho phương trình SDE} \label{Estimating-Scores-For-The-SDE}

Điểm số của một phân phối có thể được ước lượng bằng cách huấn luyện một mô hình dựa trên điểm số trên các mẫu với khớp điểm số \citep{hyvarinen2005estimation,song2020sliced}. Để ước lượng $\nabla_{\bold{x}} \log p_t(\bold{x})$, ta có thể huấn luyện một mô hình dựa trên điểm số phụ thuộc vào thời gian $\bold{s}_{\boldsymbol{\theta}}(\bold{x}, t)$ thông qua một tổng quát hóa liên tục từ phương trình \ref{eq:1} và phương trình \ref{eq:3}:

\begin{equation} \label{eq:7}
    \boldsymbol{\theta}^{\ast} = \argmin_{\boldsymbol{\theta}} \mathbb{E}_t \Big \lbrace  \lambda(t) \mathbb{E}_{\bold{x}(0)} \mathbb{E}_{\bold{x}(t) \bold{x}(0)} \big \lbrack \lVert \bold{s}_{\boldsymbol{\theta}} (\bold{x}(t), t) - \nabla_{\bold{x}(t)} \log p_{0t} (\bold{x}(t) \vert \bold{x}(0)) \rVert_2^2 \big \lbrack \Big \rbrace
\end{equation}

với $\lambda: \lbrace 0, T \rbrace \rightarrow \mathbb{R}_{>0}$ là một hàm trọng số dương,
$t$ được lấy mẫu theo phân phối đều trên đoạn $\lbrace 0, T \rbrace$, $\bold{x}(0) \sim p_0 (\bold{x})$ và $\bold{x}(t) \sim p_{0t} (\bold{x}(t) \vert \bold{x}(0))$.
Với mô hình có kích thước và lượng dữ liệu phù hợp, khớp điểm số đảm bảo lời giải tối ưu của phương trình \ref{eq:7} được ký hiệu bởi $\bold{s}_{\boldsymbol{\theta}^{\ast}}(\bold{x}, t)$ bằng với $\nabla_{\bold{x}} \log p_t (\bold{x})$ với hầu hết $\bold{x}$ và $t$.
Trong SMLD và DDPM, ta thường chọn $\lambda \propto 1 / \mathbb{E} \big \lbrack \lVert \nabla_{\bold{x}(t)} \log p_{0t} (\bold{x}(t) \vert \bold{x}(0)) \rVert_2^2 \big \rbrack$.
Ta cần chú ý rằng công thức \ref{eq:7} sử dụng khớp điểm số khử nhiễu nhưng các hàm mục tiêu khớp điểm số khác ví dụ như khớp điểm số theo lát \citep{song2020sliced} và khớp điểm số sai phân hữu hạn \citep{pang2020efficient} cũng có thể được áp dụng ở đây.

Ta thường cũng cần phải biết nhân chuyển tiếp $p_{0t} (\bold{x}(t) \vert \bold{x}(0))$ để giải phương trình \ref{eq:7} một cách hiệu quả.
Nếu $\bold{f}(.,t)$ là hàm affine, nhân chuyển tiếp luôn luôn là phân phối Gaussian mà trung bình và phương sai thường được biết trong dạng đóng và có thể thu được bằng các kỹ thuật chuẩn tắc (xem mục 5.5  trong \cite{sarkka2019applied}).
Với phương trình SDE tổng quát hơn, ta có thể giải phương trình Kolmogorov thuận \citep{oksendal2003stochastic} để thu được $p_{0t} (\bold{x}(t) \vert \bold{x}(0))$.
Một cách khác, ta có thể mô phỏng phương trình SDE để lấy mẫu từ $p_{0t}(\bold{x}(t) \vert \bold{x}(0))$ và thay thế thành phần khớp điểm số khử nhiễu trong \ref{eq:7} với khớp điểm số theo lát để huấn luyện mô hình mà bỏ qua việc tính toán $\nabla_{\bold{x}(t)} \log p_{0t} (\bold{x}(t) \vert \bold{x}(0))$ (xem phụ lục ).

\subsection{Một số ví dụ: VE, VP SDE và các SDE khác}

Nhiễu được dùng để làm xáo trộn dữ liệu trong SMLD và DDPM có thể được xem là phiên bản rời rạc hóa của hai phương trình SDE khác nhau.
Ta sẽ nói ngắn gọn về phần này và chi tiết dudocj trình bày ở phụ lục .

Khi ta sử dụng $N$ mức nhiễu, từng nhân xáo trộn dữ liệu $p_{\sigma_i}(\bold{x} \vert \bold{x}_0)$ của SMLD tương ứng với phân phối của $\bold{x}_i$ theo xích Markov sau:

\begin{equation}
    \bold{x}_i = \bold{x}_{i-1} + \sqrt{\sigma_i^2 - \sigma_{i-1}^2} \bold{z}_{i-1}, i=1, \dots, N
\end{equation}

với $\bold{z}_{i-1} \sim \mathcal{N}(\bold{0}, \bold{I})$ và ta đã đặt $\sigma_0=0$ để làm đơn giản hóa ký hiệu.
Khi $N \rightarrow \infty$, $\lbrace \sigma_i \rbrace_{i=1}^N$ trở thành một hàm $\sigma(t)$, $\bold{z}_i$ trở thành $\bold{z}_t$, và xích Markov $\lbrace \bold{x}_i \rbrace_{i=1}^N$ trở thành một quá trình ngẫu nhiên liên tục $\lbrace \bold{x}(t) \rbrace_{t=0}^1$, ta sử dụng biến thời gian liên tục $t \in \lbrack 0, 1 \rbrack$ để đánh chỉ số thay vì số nguyên $i$.
Quá trình $\lbrace \bold{x}(t) \rbrace_{t=0}^1$ được cho bởi phương trình SDE sau:

\begin{equation} \label{eq:9}
    d \bold{x} = \sqrt{\dfrac{d \lbrack \sigma^2(t) \rbrack}{dt}} d \bold{w}
\end{equation}

Tương tự như với nhân gây xáo trộn $\lbrace p_{\alpha_i} (\bold{x} \vert \bold{x}_0) \rbrace_{i=1}^N$ của DDPM, xích Markov rời rạc là:

\begin{equation} \label{eq:10}
    \bold{x}_i = \sqrt{1 - \beta_i} \bold{x}_{i-1} + \sqrt{\beta_i} \bold{z}_{i-1}, i=1,\dots,N
\end{equation}

Khi $N \rightarrow \infty$, phương trình \ref{eq:10} hội tụ về phương trình SDE sau đây:

\begin{equation} \label{eq:11}
    d \bold{x} = \dfrac{1}{2} \beta(t) \bold{x} dt + \sqrt{\beta(t)} d\bold{w}
\end{equation}

vì vậy, nhiễu dùng để xáo trộn được dùng trong SMLD và DDPM tương ứng với sự rời rạc hóa của phương trình SDE trong công thức \ref{eq:9} và \ref{eq:11}.
Một điều thú vị là phương trình SDE của công thức \ref{eq:9} luôn luôn tạo ra một quá trình với phương sai bùng nổ khi $t \rightarrow \infty$, trong khi phương trình ở công thức \ref{eq:11} thu được một quá trình với phương sai không đổi là một khi phân phối ban đầu có phương sai là một (chứng minh ở phụ lục ).
Vì sự khác biệt này, kể từ đây ta sẽ gọi phương trình ở công thức \ref{eq:9} là Variance Exploding (VE) SDE, và phương trình ở công thức \ref{eq:11} là Variance Preserving (VP) SDE.

Lấy cảm hứng từ VP SDE, ta đề xuất một loại phương trình SDE mới đặc biệt tốt trên độ hợp lý (mục ) được cho bởi công thức:

\begin{equation} \label{eq:12}
    d \bold{x} = -\dfrac{1}{2} \beta(t) \bold{x} dt + \sqrt{\beta(t) (1-e^{-2\int_{0}^{t} \beta(s)ds} )} d \bold{w}
\end{equation}

Khi sử dụng cùng $\beta(t)$ và bắt đầu từ cùng một phân phối ban đầu, phương sai của quá trình ngẫu nhiên thu được từ phương trình \ref{eq:12} luôn luôn bị chặn bởi VPSDE tại mỗi bước thời gian trung gian (chứng minh ở phục lục ).
Vì lý do này, ta đặt tên công thức \ref{eq:12} là sub-VP SDE.

Khi VE, VP, sub-VP SE đều có một độ trôi, nhân làm xáo trộn dữ liệu $p_{0t}(\bold{x}(t) \vert \bold{x}(0))$ tất cả đều có phân phối Gaussian và có thể được tính ở dạng đóng đã được trình bày ở mục \ref{Estimating-Scores-For-The-SDE}.
Điều này làm việc huấn luyện mô hình với công thức \ref{eq:7} đặc biệt hiệu quả.

\section{Giải phương trình SDE ngược}

Sau khi huấn luyện một mô hình dựa trên điểm số phụ thuộc vào thời gian $\bold{s}_{\boldsymbol{\theta}}$, ta có thể sử dụng mô hình này để xây dựng một phương trình SDE đảo ngược thời gian và sau đó mô phỏng nó với cách tiếp cận phuowg pháp số để sinh ra các mẫu dữ liệu từ phân phối $p_0$.

\subsection{Bộ giải phương trình SDE bằng phương pháp số đa chức năng}

\begin{table}[h!]
    \centering
    \caption{So sánh các bộ giải phương trình SDE đảo ngược thời gian khác nhau trên tập CIFAR-10.
    Vùng tối màu thu được với cùng cách tính toán (số các hàm điểm số).
    Trung bình và độ lệch tiêu chuẩn được đưa ra trên 5 lần chạy.
    "P1000" hoặc "P2000": bộ lấy mẫu mà chỉ dự đoán sử dụng 1000 hoặc 2000 bước.
    "C2000": bộ lấy mẫu chỉ căn chỉnh sử dụng 2000 bước.
    "PC1000": Bộ lấy mẫu dự đoán căn chỉnh sử dụng 1000 bước dự đoán và 1000 bước căn chỉnh.}
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{c|c|c|c|c|c|c|c|c}
            \hline
            & \multicolumn{4}{|c|}{Variance Exploding SDE (SMLD)} & \multicolumn{4}{|c}{Variance Preserving SDE (DDPM)} \\
            \hline
            FID $\downarrow$ & P1000 & P2000 & C2000 & PC1000 & P1000 & P2000 & C2000 & PC1000 \\
            \hline
            ancestral sampling & 4.98 $\pm$ .06 & 4.88 $\pm$ .06 & \multirow{3}{*}{20.43 $\pm$ .07} & \textbf{3.62} $\pm$ \textbf{.03} & 3.24 $\pm$  .02 & 3.24 $\pm$ .02 & \multirow{3}{*}{19.06 $\pm$ .06} & \textbf{3.21} $\pm$ \textbf{.02} \\
            reverse diffusion & 4.79 $\pm$ .07 & 4.74 $\pm$ .08 & & \textbf{3.60} $\pm$ \textbf{.02} & 3.21 $\pm$  .02 & 3.19 $\pm$ .02 & & \textbf{3.18} $\pm$ \textbf{.01} \\
            probability flow & 15.41 $\pm$ .15 & 10.54 $\pm$ .08 & & \textbf{3.51} $\pm$ \textbf{.04} & 3.59 $\pm$  .04 & 3.23 $\pm$ .03 & & \textbf{3.06} $\pm$ \textbf{.03} \\
            \hline
        \end{tabular}
    }
    \label{tab:1}
\end{table}

Các bộ giải sử dụng phương pháp số tạo ra một quỹ đạo xấp xỉ từ phương trình SDE.
Tồn tại nhiều phương pháp số đa chức năng để giải phương trình SDE như Euler-Maruyama và phương pháp Runge-Kutta ngẫu nhiên, tương ứng với các cách rời rạc hóa khác nhau của động học ngẫu nhiên.
Ta có thể áp dụng bất kỳ cách nào cho phương trình SDE đảo ngược thời gian để sinh mẫu dữ liệu.

Ancestral sampling, là phương pháp lấy mẫu của DDPM trong công thức \ref{eq:4} thực tế tương ứng với một phương pháp rời rạc hóa đặc biệt của phương trình VP SDE đảo ngược thời gian (công thức \ref{eq:11}) (phụ lục ).
Để thu được luật lấy mẫu sử dụng ancestral sampling cho phương trình SDE mới không phải là việc dễ dàng.
Để khắc phụ điều này, ta đề xuất \textit{bộ lấy mẫu khuếch tán ngược} (chi tiết trong phụ lục ), bộ lấy mẫu này rời rạc hóa phương trình SDE đảo ngược thời gian theo cùng cách với phương trình thuận, và vì vậy có thể đễ dàng thực hiện sử dụng kết quả rời rạc hóa của quá trình thuận
Hình \ref{tab:1}, bộ lấy mẫu khuếch tán ngược thể hiện tốt hơn một chút so với ancestral sampling cho cả hai mô hình SMLD và DDPM trên CIFAR-10 (mô hình DDPM sử dụng ancestral sampling cũng có thể áp dụng cho mô hình SMLD, xem phụ lục ).

\subsection{Bộ lấy mẫu dự đoán - căn chỉnh}

Không giống với phương trình SDE tổng quát, ta có thêm thông tin có thể sử dụng để cải thiện lời giải.
Khi ta có một mô hình dựa trên điểm số $\bold{s}_{\boldsymbol{\theta}^{\ast}} (\bold{x}, t)\approx \nabla_{\bold{x}} \log p_t (\bold{x})$, ta có thể sử dụng cách tiếp cận MCMC dựa trên điểm số ví dụ như Langevin MCMC \citep{parisi1981correlation,grenander1994representations} hoặc HMC \citep{neal2011mcmc} để lấy mẫu trực tiếp từ $p_t$ và căn chỉnh lời giải của một bộ giải phương trình SDE bằng phương pháp số.

Cụ thể, tại từng bước thời gian, bộ giải phương trình SDE bằng phương pháp số ban đầu tạo ra một ước lượng của mẫu tại bước tiếp theo, đóng vai trò là một "bộ dự đoán".
Sau đó, ta dùng MCMC dựa trên điểm số căn chỉnh phân phối cận biên của mẫu đã được ước lượng, đóng vai trò là "bộ căn chỉnh".
Ý tưởng tương tự với phương pháp dự đoán - căn chỉnh, một họ của các kỹ thuật phương pháp số cho giải hệ phương trình \citep{allgower2012numerical} và ta đặt một tên tương tự cho thuật toán lấy mẫu là bộ lấy mẫu \textit{dự đoán - căn chỉnh}.
Mã giả được được trình bày ở phụ lục .
Bộ lấy mẫu PC tổng quát hóa các phương pháp lấy mẫu ban đầu của SMLD và DDPM: các phương pháp trước đây sử dụng một hàm đồng nhất là hàm dự đoán và tôi luyện động học Langevin làm bộ căn chỉnh, trong khi các phương pháp sau sử dụng ancestral sampling là bộ dự đoán và hàm đồng nhất là bộ căn chỉnh.

Ta sẽ thử bộ lấy mẫu PC trên các mô hình SMLD và DDPM (thuật toán và thuật toán trong phụ lục) được huấn luyện với hàm mục tiêu rời rạc ban đầu trong các công thức \ref{eq:1} và \ref{eq:3}.
Các công thức này cho ta thấy sự phù hợp của bộ lấy mẫu PC với mô hình dựa trên điểm số được huấn luyện với một số bước cố định các mức nhiễu.
Ta tổng hợp khả năng của các bộ lấy mẫu khác nhau trong bảng \ref{tab:1}, với dòng xác suất là một bộ dự đoán sẽ được trình bày ở trong mục \ref{Probability-Flow-and-Connection-to-Neural-ODEs}.
Cấu hình chi tiết về các thí nghiệm và các kết quả chi tiết hơn được trình phù ở phụ lục.
Ta quan sát thấy rằng bộ lấy mẫu khuếch tán ngược luôn luôn tốt hơn so với các phương pháp ancestral sampling, chỉ căn chỉnh (C2000) và kém hơn so với các bộ lấy mẫu khác (PC2000, PC1000) cùng với lượng tính toán.
(Thực tế, ta cần nhiều bước căn chỉnh hơn trên một mức nhiễu, và vì vậy cần tính toán nhiều hơn để khớp với khả năng của các bộ lấy mẫu khác)
Với tất cả các bộ dự đoán, thêm một bước căn chỉnh cho từng bước dựa đoán (PC1000) làm tăng khối lượng tính toán lên gấp đôi nhưng luôn luôn cải thiện chất lượng mẫu (so với PC1000).
Hơn nữa, thông thường việc thêm một bước dự đoán và căn chỉnh thường tốt hơn tăng gấp đôi số bước dự đoán mà không thêm bước căn chỉnh (P2000),
khi ta luôn phải nội suy giữa những mức nhiễu (chi tiết trong phụ lục) cho các mô hình SMLD/DDPM.
Trong hình (phụ lục), ta cung cấp thêm so sánh về mặt chất các mô hình cho các hàm mục tiêu trong công thức \ref{eq:7} trên tập LSUN 256 $\times$ 256 và VE SDE, với các bộ lấy mẫu PC vượt trội hơn hẳn các bộ lấy mẫu chỉ dự đoán với cùng một khối lượng tính toán khi ta dùng một số lượng phù hợp các bước căn chỉnh.

\subsection{Dòng xác suất và sự liên kết với các phương trình ODE neuron} \label{Probability-Flow-and-Connection-to-Neural-ODEs}

Các mô hình dựa trên điểm số cho phép phương pháp số khác nhau để giải các phương trình SDE đảo ngược thời gian.
Với tất cả các quá trình khuếch tán, tồn tại một quá trình tất định tương ứng mà quỹ đạo này chia sẻ cùng một hàm mật độ xác suất cận biên $\lbrace p_t (\bold{x}) \rbrace_{t=0}^T$ cùng với phương trình SDE.
Quá trình tất định này thỏa mãn một phương trình ODE (chi tiết được trình bày trong phục lục ):

\begin{equation} \label{eq:13}
    d \bold{x} = \Big \lbrack \bold{f}(\bold{x}, t) - \dfrac{1}{2}g(t)^2 \nabla_{\bold{x}} \log p_t (\bold{x}) \Big \rbrack dt
\end{equation}

có thể được xác định từ một phương trình SDE khi điểm số đã được biết.
Ta đặt tên ODE trong công thức \ref{eq:13} là phương trình ODE dòng xác suất.
Khi hàm điểm số được xấp xỉ bởi một mô hình dựa trên điểm số phụ thuộc vào thời gian, thường là một mạng neuron là một ví dụ của một ODE neuron \citep{chen2018neural}.

\textbf{Tính toán chính xác độ hợp lý} Tận dụng sự liên kết với các phương trình ODE neuron, ta có thể tính hàm mật độ được định nghĩa bởi công thức \ref{eq:13} thông qua sự thay đổi tức thời của các biến trong công thức \citep{chen2018neural}.
Điều này cho phép ta tính toán được \textit{chính xác độ hợp lý trên bất kỳ dữ liệu đầu vào nào} (chi tiết trong phụ lục).
Một ví dụ, ta báo cáo hàm âm log của độ hợp lý (negative log-likelihoods - NLLs) được đo trên bits/dim trên tập CIFAR-10 trong bảng \ref{tab:2}.
Ta tính log của độ hợp lý một cách đồng đều trên dữ liệu đã được giải lượng tự và chỉ so sánh với có các mô hình được đánh giá theo cùng một cách (bỏ qua các mô hình được đánh giá với giải lượng tử hóa biến phân \citep{ho2019flow++} hoặc ữ liệu rời rạc), ngoại trừ với DDPM (L/$L_{\mathrm{simple}}$) mà giá trị ELBO (được gán nhãn với $\ast$) được báo cáo trên tập rời rạc.
Các kết quả chính: (i) Với cùng mô hình DDPM trong \citep{ho2020denoising}, ta thu được bits/dim cao hơn ELBO, khi độ hợp lý của ta là chính xác;
(ii) Sử dụng cùng cấu trúc, ta huấn luyện mô hình DDPM khác với hàm mục tiêu liên tục trong công thức \ref{eq:7} cũng cải thiện thêm độ hợp lý;
(iii) với sub-VP SDEs, ta luôn luôn thu được độ hợp lý cao hơn so với VP SDEs;
(iv) với kiến trúc được cải thiện (ví dụ DDPM++ ở mục ) và sub-VP SDE, ta có thể lập một kỷ lục mới bits/dim là 2.99 trên tập CIFAR-10 giải lượng tử đều ngay cả không sử dụng \textit{huấn luyện cực đại hóa độ hợp lý}.


\begin{table}[h!]
    \centering
    \caption{NLLs và FIDs (ODE) trên tập CIFAR-10}
    \begin{tabular}{lcc}
        \hline
        Model & NLL Test $\downarrow$ & FID $\downarrow$ \\
        \hline
        RealNVP \citep{dinh2016density} & 3.49 & - \\
        iResNet \citep{behrmann2019invertible} & 3.45 & - \\
        Glow \citep{kingma2018glow} & 3.35 & - \\
        MintNet \citep{song2019mintnet} & 3.32 & - \\
        Residual Flow \citep{chen2019residual} & 3.28 & 46.37 \\
        FFJORD \citep{grathwohl2018ffjord} & 3.40 & - \\
        Flow++ \citep{ho2019flow++} & 3.29 & - \\
        DDPM (L) \cite{ho2020denoising} & $\leq 3.70^{\ast}$  & 13.51 \\
        DDPM ($L_{\mathrm{simple}}$) \citep{ho2020denoising} & $\leq 3.75^{\ast}$ & 3.17 \\
        \hline
        DDPM & 3.28 & 3.37 \\
        DDPM cont. (VP) & 3.21 & 3.69 \\
        DDPM cont. (sub-VP) & 3.05 & 3.56 \\
        DDPM++ cont. (VP) & 3.16 & 3.93 \\
        DDPM++ cont. (sub-VP) & 3.02 & 3.16 \\
        DDPM++ cont. (deep, VP) & 3.13 & 3.08 \\
        DDPM++ cont. (deep, sub-VP) & \textbf{2.99} & \textbf{2.92} \\
        \hline
    \end{tabular}
    \label{tab:2}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Chất lượng các mẫu trên tập CIFAR-10}
    \begin{tabular}{lcc}
        \hline
        Model & FID $\downarrow$ & IS $\uparrow$ \\
        \hline
        \textbf{Conditional} & & \\
        BigGAN \citep{brock2018large} &  14.73 & 9.22 \\
        StyleGAN2-ADA \citep{karras2020training} & \textbf{2.42} & \textbf{10.14} \\
        \hline
        \textbf{Unconditional} & & \\
        StyleGAN2-ADA \citep{karras2020training} & 2.92 & 9.83 \\
        NCNS \citep{song2019generative} & 25.32 & 8.87 $\pm$ .12 \\
        NCNSv2 \citep{song2020improved} & 10.87 & 8.40 $\pm$ .07 \\
        DDPM \citep{ho2020denoising} & 3.17 & 9.46 $\pm$ .11 \\
        \hline
        DDPM++ & 2.78 & 9.64 \\
        DDPM++ cont. (VP) & 2.55 & 9.58 \\
        DDPM++ cont. (sub-VP) & 2.61 & 9.56 \\
        DDPM++ cont. (deep, VP) & 2.41 & 9.68 \\
        DDPM++ cont. (deep, sub-VP) & 2.41 & 9.57 \\
        NCSN++ & 2.45 & 9.73 \\
        NCSN++ cont. (VE) & 2.38 & 9.83 \\
        NCSN++ cont. (deep, VE) & \textbf{2.20} & \textbf{9.89} \\
        \hline
    \end{tabular}
    \label{tab:3}
\end{table}

\textbf{Thao tác trên biểu diễn ẩn} Bằng cách kết hợp công thức \ref{eq:13}, ta có thể mã hóa bất kỳ điểm dữ liệu $\bold{x}(0)$ nào vào một không gian ẩn $\bold{x}(T)$.
Giải mã có thể thu được bằng cách kết hợp phương trình ODE tương ứng cho phương trình SDE đảo ngược thời gian.
Như đã được thực hiện với các mô hình có thể đảo ngược khác như ODEs neuron và dòng chuẩn hóa \citep{dinh2016density, kingma2018glow}, ta có thể thao tác biểu diễn này cho chỉnh sửa ảnh, như nội suy và chia tỷ lệ nhiệt độ.

\textbf{Mã hóa có thể nhận dạng duy nhất} Không giống như hầu hết các mô hình có thể đảo ngược hiện tại,
cách mã hóa của ta là có thể nhận diện duy nhất, nghĩa là với dữ liệu huấn luyện phù hợp, kích thước mô hình và độ chính xác tối ưu, mã hóa cho một input được xác định duy nhất bởi phân phối dữ liệu \citep{roeder2021linear}.
Bởi vì phương trình SDE thuận, công thức \ref{eq:5} không có tham số huấn luyện được, và tương ứng với dòng xác suất ODE, công thức \ref{eq:13} cho ta cùng quỹ đạo khi cho ước lượng điểm số hoàn hảo.
Ta cung cấp các kiểm chứng thực nghiệm bổ sung về tính chất này ở phục lục .

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/3.png}
    \caption{\textbf{ODE dòng xác suất cho phép lấy mẫu nhanh} với độ dài bước thích nghi và độ chính xác số học thích nghi thay đổi (\textit{hình trái}), giảm số lần đánh giá hàm điểm số (NFE) mà không làm ảnh hưởng đến chất lượng (\textit{hình giữa}).
    Ánh xạ ngược từ không gian ẩn sang ảnh cho phép nội suy (\textit{hình phải})}
    \label{fig:3}
\end{figure}

\textbf{Lấy mẫu hiệu quả} Với một ODE neuron, ta có thể lấy mẫu $\bold{x}(0) \sim p_0$ bằng cách giải phương trình ở công thức \ref{eq:13} từ các điều kiện $\bold{x}(T) \sim p_T$.
Sử dụng một chiến lược rời rạc hóa cố định ta có thể tạo ra các mẫu có khả năng cạnh tranh cao, đặc biệt khi sử dụng cùng vơi shbooj hiệu chỉnh (bảng \ref{tab:1}, "bộ lấy mẫu dòng xác suất", chi tiết ở phụ lục).
Sử dụng một bộ giải phương trình ODE dạng hộp đen \citep{dormand1980family} không những chỉ tạo ra những mẫu chất lượng cao (\ref{tab:2}, chi tiết ở phụ lục ) mà còn cho phép ta đánh đổi độ chính xác cho tính hiệu quả.
Với khả năng chống chịu sai lệch cao hơn, số hàm đánh giá có thể được giảm hơn 90 \% mà không cần làm ảnh hưởng chất lượng của mẫu (hình \ref{fig:3})

\subsection{Các cải thiện về kiến trúc của mô hình}

Ta khám phá nhiều thiết kế kiến trúc mới cho mô hình dựa trên điểm số sử dụng cả VE và VP SDEs (chi tiết ở phụ lục ), với ta huấn luyện các mô hình với các hàm mục tiêu rời rạc như SMLD/DDPM.
Ta áp dụng trực tiếp các kiến trúc của VP SDEs cho sub-VP SDEs vì tính tương đồng của hai mô hình này.
Kiến trúc tối ưu ta thấy cho VE SDE được gọi là NCSN++ thu được FID là 2.45 trên CIFAR-10, với bộ lấy mẫu PC, trong khi kiến trúc tối ưu cho VP SDE được gọi là DDPM++ đạt được kết quả với FID là 2.78.

Bằng cách chuyển sang sử dụng công thức huấn luyện liên tục ở công thức \ref{eq:7} và tăng độ sâu của mạng, ta có thể cải thiện hơn nữa chất lượng của mẫu cho tất cả các mô hình.
Các kiến trúc được chọn được ký hiệu là NCSN++ cont. và DDPM++ cont. trong bảng \ref{tab:3} lần lượt cho VE và VP/sub-VP SDEs.
Các kết quả được báo cáo trong bảng \ref{tab:3} tương ướng với các checkpoint với FID nhỏ nhất trong quá trình huấn luyện, với các mẫu được tạo ra với bộ lấy mẫu PC.
Ngược lại, chỉ số FID và các giá trị NLL trong bảng \ref{tab:2} được báo cáo cho checkpoint cuối cùng trong quá trình huấn luyện, và các bộ lấy mẫu thu được với một bộ giải phương trình ODE là một hộp đen.
Như được trình bày trong b ảng \ref{tab:3}, VE SDEs thường cho các mẫu có chất lượng tốt hơn VP/sub-VP SDE.
Điều này cho thấy những người người làm thực tiễn có xu hướng thích làm thí nghiệm với các phương trình ngẫu nhiên SDEs khác nhau cho nhiều phạm vi dữ liệu và kiến trúc.

Mô hình cho mẫu chất lượng tốt nhất là NCSN++ cont. (deep, VE), có chiều sâu gấp đôi và tạo kỷ lục mới cho cả inception score và FID cho bài toán sinh mẫu không có điều kiện trên tập CIFAR-10.
Điều khá ngạc nhiên là ta có thể đạt được FID tốt hơn các mô hình sinh có điều kiện tốt nhất trước đây mà không cần dữ liệu được gán nhãn.
Kết hợp tất cả các cải tiến lại, ta cũng thu được tập mẫu với độ chân thực cao trên tập CelebA-HQ 1024 $\times$ 1024 từ các mô hình dựa trên dựa trên điểm số (phụ lục ).
Mô hình tốt nhất của ta cho độ hợp lý là DDPM++ cont. (deep, sub VP), cũng chiều sâu gấp đôi và thu được log của độ hợp lý là 2.99 bits/dim với hàm mục tiêu liên tục trong công thức \ref{eq:7}.
Với những gì ta biết, đây là độ hợp lý cao nhất trên tập CIFRA-10 được lấy mẫu đều.

\section{Sinh mẫu có điều khiển}

Cấu trúc liên tục của khung làm việc của ta cho phép ta không chỉ sinh ra các mẫu dữ liệu từ phân phối $p_0$ mà còn từ phân phối $p_0 (\bold{x}(0) \vert \bold{y})$ nếu $p_t (\bold{y} \vert \bold{x}(t))$ biết.
Khi được cho trước một phương trình SDE thuận trong công thức \ref{eq:5}, ta có thể lấy mẫu từ $p_t(\bold{x}(t) \vert \bold{y})$ bằng cách bắt đầu từ $p_T (\bold{x}(T) \vert \bold{y})$ và giải phương trình SDE đảo ngược thời gian có điều kiện:

\begin{equation} \label{eq:14}
    d \bold{x} = \lbrace \bold{f}(\bold{x}, t) - g (t)^2 \lbrack \nabla_{\bold{x}} \log p_t(\bold{x}) + \nabla_{\bold{x}} \log p_t (\bold{y} \vert \bold{x}) \rbrack \rbrace dt + g(t) d \bar{\bold{w}}
\end{equation}

Tổng quát, ta có thể sử dụng công thức \ref{eq:14} để giải một lớp lớn các \textit{bài toán ngược} với mô hình sinh dựa trên điểm số,
một khi được biết một ước lượng của gradient của quá trình thuận $\nabla_{\bold{x}} \log p_t (\bold{y} \vert \bold{x}(t))$.
Trong một số trường hợp, ta có thể huấn luyện một mô hình riêng biệt để học quá trình thuận $p_t(\bold{y} \vert \bold{x}(t))$ và tính gradient của quá trình này.
Mặt khác, ta có thể ước lượng gradient với kinh nghiệm và kiến thức chuyên gia về một miền dữ liệu.
Ở phụ lục , ta cung cấp các phương pháp được ứng dụng rộng rãi để thu được những ước lượng mà không cần huấn luyện mô hình phụ.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/4.png}
    \caption{\textit{Hình trái:} Các mẫu được tạo ra dựa trên điều kiện về lớp trên tập CIFAR-10 32 $\times$ 32.
    Bốn hàng đầu tiên là oto và bốn hàng cuối là ngựa.
    \textit{Hình phải:} Bài toán inpainting (hai hàng đầu) và bài toán tô màu (hai hàng cuối) kết quả trên tập LSUN 256 $\times$ 256.
    Cột đầu tiên là ảnh gốc, cột thứ hai là ảnh bị gán mặt nạ hoặc chuyển sang ảnh xám, các cột còn lại là các mẫu ảnh được hoàn thiện hoặc được tô màu.}
    \label{fig:4}
\end{figure}

Ta xét ba ứng dụng của bài toán sinh mẫu có điều khiển với các cách tiếp cận:
sinh mẫu có điều kiện dựa trên lớp, xóa một phần ảnh và tô màu.
Khi $\bold{y}$ biểu diễn nhãn lớp, ta có thể huấn luyện một bộ phân loại phụ thuộc vào thời gian $p_t(\bold{y} \vert \bold{x}(t))$ cho bài toán lấy mẫu điều kiện trên lớp.
Khi phương trình thuận SDE dễ giải, ta có thể dễ dàng tạo ra dữ liệu huấn luyện $(\bold{x}(t), \bold{y})$ cho bộ phân loại phụ thuộc vào thời gian bởi lấy mẫu ban đầu $\bold{x}(0), \bold{y}$ từ tập dữ liệu và sau đó lấy mẫu $\bold{x}(t) \sim p_{0t} (\bold{x}(t) \vert \bold{x}(0))$.
Sau đó, ta có thể sử dụng một sự kết ợp của các hàm mất mát cross-entropy trên các bước thời gian khác nhau, như công thức \ref{eq:7}, để huấn luyện bộ phân loại phụ thuộc theo thời gian $p_t(\bold{y} \vert \bold{x}(t))$.
Ta cung cấp các mẫu được sinh ra có điều kiện dựa trên lớp trên tập CIFAR-10 ở hình \ref{fig:4} (hình trái) và các thông tin chi tiết hơn về kết quả ở phụ lục .

Xóa một phần thông tin ảnh là một trường hợp đặc biệt của lấy mẫu có điều kiện.
Giả sử ta có một điểm dữ liệu không hoàn chỉnh $\bold{y}$ với chỉ một tập con, $\Omega(\bold{y})$ biết trước.
Lượng dữ liệu bị xóa từ ảnh để lấy mẫu từ $p(\bold{x}(0) \vert \Omega(\bold{y}))$, ta có thể thực hiện sử dụng một mô hình sinh không có điều kiện (xem phụ lục ).
Bài toán tô màu là một trường hợp đặc biệt của xóa một phần thông tin ảnh, ngoại trừ chiều dữ liệu đã biết.
Ta có thể chiều dữ liệu với một phép biến đổi tuyến tính trực giao và thực hiện xóa thông tin ảnh trong không gian đã bị biến đổi (chi tiết ở trong phụ lục ).
Hình \ref{fig:4} (bên phải) cho biết kết quả cho bài toán xóa thông tin ảnh và tô màu thu được với mô hình dựa trên điểm số phụ thuộc vào thời gian không có điều kiện.

\section{Kết luận}

Ta đã trình bày một khung làm việc cho các mô hình sinh dựa trên điểm số trên các phương trình vi phân ngẫu nhiên SDEs.
Công trình của ta cho phép việc hiểu sâu hơn của các cách tiếp cận đã tồn tại, các thuật toán lấy mẫu mới, tính chính xác độ hợp lý, mã hóa có thể nhận dạng duy nhất, thao tác trên không gian ẩn và mang lại khả năng sinh dữ liệu của mô hình sinh dựa trên điểm số.

Trong khi đó phương pháp tiếp cận ta đề xuất cải thiện các kết quả và cho phép lấy mẫu hiệu quả hơn, nhưng tốc độ lấy mẫu vẫn chậm hơn GANs \citep{goodfellow2020generative} trên cùng các tập dữ liệu.
Nhận biết các cách kết hợp quá trình huấn luyện ổn định của các mô hình sinh dựa trên điểm số kết hợp với lấy mẫu nhanh của các mô hình ngầm định như GANs vẫn là một hướng nghiên cứu quan trọng.
Hơn nữa, độ rộng của bộ lấy mẫu có thể sử dụng khi biết số siêu tham số của hàm điểm số.
các công trình trong tương lai có thể tận dụng từ các phương pháp đã được cải thiện để tự động lựa chọn và tinh chỉnh các siêu tham số cũng như các khám phá mở rộng dựa trên những giá trị và những hạn chế của nhiều phương pháp lấy mẫu khác nhau.
\newpage
\bibliography{iclr2021_conference}
\bibliographystyle{iclr2021_conference}

\newpage

\appendix

Ta trình bày một số phụ lục với các chi tiết bổ sung, chứng minh công thức và kết quả.
Khung làm việc của ta cho phép phương trình SDE tổng quan với các hệ số khuếch tán có giá trị dạng ma trận phụ thuộc vào trạng thái, trong đó ta cung cấp một số nhận xét chi tiết trong phụ lục .
Ta cung cấp chứng minh chi tiết của VE, VP, và sub-VP SDEs trong phụ lục , và ta thảo luận cách sử dụng từ quan điểm thực tiễn ở phụ lục .
Ta xây dựng công thức dòng xác suất trong phụ lục , bao gồm chứng minh thu được công thức dòng xác suất của phương trình ODE (phụ lục ), tính toán chính xác độ hợp lý (phụ lục ), lấy mẫu dòng xác suất với chiến lược rời rạc hóa cố định (phụ lục ),
lấy mẫu bằng bộ giải phương trinh ODE là một hộp đen (phụ lục ), và xác minh các thử nghiệm trên mã hóa có thể nhận dạng duy nhất (phụ lục ).
Ta cung cấp mô tả đầy đủ về bộ lấy mẫu quá trình khuếch tán ngược trong phụ lục ,
bộ lấy mẫu ancestral loại DDPM cho mô hình SMLD trong phụ lục , và bộ lấy mẫu dự đoán - hiệu chỉnh trong phụ lục .
Ta giải thích các kiến trúc của mô hình và các cài đặt thử nghiệm chi tiết trong phụ lục ,
với các mẫu độ phân giải 1024 $\times$ 1024 trên tập CelebA-HQ.
Cuối cùng, ta trình bày chi tiết về các thuật toán sinh mẫu có điều khiển trong phụ lục  và bao gồm các kết quả mở rộng cho sinh dữ liệu dựa trên lớp (phụ lục ),
bài toán tô màu (phụ lục ) và chiến lực cho giải bài toán ngược tổng quát (phụ lục ).

\section{Khung làm việc cho phương trình SDE tổng quát}

Trong phần nội dung chính, ta đã giới thiệu khung làm việc dựa trên phương trình SDE ở công thức \ref{eq:5} với hệ số khuếch tán độc với $\bold{x}(t)$.
Cho thấy rằng khung làm việc của ta có thể mở rộng với những tham số khuếch tán tổng quát hơn.
Ta có thể xem xét phương trình SDEs với dạng sau:

\begin{equation} \label{eq:15}
    d \bold{x} = \bold{f} (\bold{x}, t) dt + \bold{G} (\bold{x}, t) d\bold{w}
\end{equation}

với $\bold{f}(., t): \mathbb{R}^d \rightarrow \mathbb{R}^d$ và $\bold{G}(., t): \mathbb{R}^d \rightarrow \mathbb{R}^{d \times d}$.
Ta sử dụng giải thích Ito về SDEs xuyên suốt công trình này.

Theo \citep{anderson1982reverse}, phương trình SDE đảo ngược thời gian được cho bởi công thức (công thức \ref{eq:6}):

\begin{equation} \label{eq:16}
    d \bold{x} = \lbrace \bold{f}(\bold{x}, t) - \nabla . \lbrack \bold{G}(\bold{x}, t) \bold{G}(\bold{x}, t)^T \rbrack - \bold{G}(\bold{x}, t) \bold{G}(\bold{x}, t)^T \nabla_{\bold{x}} \log p_t(\bold{x}) \rbrace dt + \bold{G}(\bold{x}, t) d\bar{\bold{w}}
\end{equation}

với ta định nghĩa $\nabla . \bold{F}(\bold{x}) := (\nabla. \bold{f}^1 (\bold{x}), \nabla. \bold{f}^2 (\bold{x}), \dots, \nabla. \bold{f}^d (\bold{x}))^T$ cho hàm giá trị ma trận $\bold{F}(\bold{x}) := (\bold{f}^1 (\bold{x}), \bold{f}^2 (\bold{x}), \dots, \bold{f}^d (\bold{x}))^T$ xuyên suốt công trình.

Dòng xác suất phương trình ODE tương ứng với công thức \ref{eq:15} có dạng sau (công thức \ref{eq:13}, chứng minh ở phụ lục ):

\begin{equation} \label{eq:17}
    d \bold{x} = \Big \lbrace \bold{f}(\bold{x}, t) - \dfrac{1}{2} \nabla . \lbrack \bold{G}(\bold{x}, t) \bold{G}(\bold{x}, t)^T  \rbrack - \bold{G}(\bold{x}, t) \bold{G}(\bold{x}, t)^T \nabla_{x} \log p_t(\bold{x}) \Big \rbrace dt
\end{equation}

Cuối cùng, cho sinh mẫu có điều kiện với phương trình SDE tổng quát \ref{eq:15}, ta có thể giải phương trình SDE đảo ngược thời gian có điều kiện dưới đây (công thức \ref{eq:14}, chi tiết ở phục lục ):\

\begin{equation} \label{eq:18}
    \begin{aligned}
        d \bold{x} = \Big \lbrace \bold{f}(\bold{x}, t) - \dfrac{1}{2} \nabla . \lbrack \bold{G}(\bold{x}, t) \bold{G}(\bold{x}, t)^T  \rbrack &- \bold{G}(\bold{x}, t) \bold{G}(\bold{x}, t)^T \nabla_{x} \log p_t(\bold{x}) \\
        & -\bold{G}(\bold{x}, t) \bold{G}(\bold{x}, t)^T \nabla_{x} \log p_t(\bold{y} \vert \bold{x})  \Big \rbrace dt + \bold{G}(\bold{x}, t) d \bar{\bold{w}}
    \end{aligned}
\end{equation}

Khi độ trôi và tham số phân tán của một phương trình SDE là không affine, có thể rất khó để tính nhân chuyển tiếp $p_{0t}(\bold{x}(t) \vert \bold{x}(0))$ ở dạng đóng.
Điều này gây trở ngại quá trình huấn luyện của các mô hình dựa trên điểm số, vì công thức \ref{eq:7} cần biết $\nabla_{\bold{x}(t)} \log p_{0t} (\bold{x}(t) \vert \bold{x}(0))$.
Để vượt qua khó khăn này, ta cần thay thế hàm khớp điểm số khử nhiễu trong công thức \ref{eq:7} với các biến thể hiệu quả khác của hàm khớp điểm số không cần tính $\nabla_{\bold{x}(t)} \log p_{0t} (\bold{x}(t) \vert \bold{x}(0))$.
Ví dụ, khi sử dụng hàm khớp điểm số theo lát \citep{song2020sliced}, hàm mục tiêu huấn luyến luyện của ta trong công thức \ref{eq:7} trở thành:


\begin{equation} \label{eq:19}
    \bold{\theta}^{\ast} = \argmin_{\bold{\theta}} \mathbb{E}_t \Bigg \lbrace \lambda (t) \mathbb{E}_{\bold{x}(0)} \mathbb{E}_{\bold{x}(t)} \mathbb{E}_{\bold{v} \sim p_{\bold{v}}} \Bigg \lbrack \dfrac{1}{2} \lVert \bold{s}_{\bold{\theta}} (\bold{x}(t), t) \rVert_2^2 + \bold{v}^T \bold{s}_{\bold{\theta}} (\bold{x}(t), t) \bold{v} \Bigg \rbrack \Bigg \rbrace
\end{equation}

với $\lambda: \lbrack 0, T \rbrack \rightarrow \mathbb{R}^{+}$ là một hàm trọng số dương, $t \sim \mathcal{U}(0, T), \mathbb{E} \lbrack \bold{v} \rbrack = \bold{0}$
Ta có thể luôn luôn mô phỏng SDE để lấy mẫu từ $p_{0t}(\bold{x}(t) \vert \bold{x}(0))$, và giải phương trình ở công thức \ref{eq:19} để huấn luyện mô hình dựa trên điểm số phụ thuộc vào thời gian $\bold{x}_{\bold{theta}}(\bold{x}, t)$.

\section{VE, VP và sub-VP SDEs}

Tiếp theo ta cung cấp các dẫn xuất công thức để chỉ ra dùng nhiễu xáo trộn của các mô hình SMLD và DDPM là phiên bản rời rạc hóa của Variance Exploding (VE) và Variance Preserving (VP) SDE tương ứng.
Ta cũng giới thiệu thêm sub-VP SDEs, một phiên bản sửa đổi của VP SDEs thường thu được kết quả tốt hơn ở cả chất lượng mẫu và độ hợp lý.

Đầu tiên, ta sử dụng tổng cộng N ngưỡng nhiễu, từng nhân xáo trộn $p_{\sigma_i} (\bold{x} \vert \bold{x}_0)$ của mô hình SMLD có thể thu được từ xích Markov:

\begin{equation} \label{eq:20}
    \bold{x}_i = \bold{x}_{i-1} + \sqrt{\sigma_i^2 - \sigma_{i-1}^2} \bold{z}_{i-1}, i = 1, \dots, N
\end{equation}

với $\bold{z}_{i-1} \sim \mathcal{N}(\bold{0}, \bold{I}), \bold{x}_0 \sim p_{\mathrm{data}}$ và ta đặt $\sigma_0 = 0$ để đơn giản hóa ký hiệu.
Trong giới hạn $N \rightarrow \infty$, xích Markov $\lbrace \bold{x}_i \rbrace_{i=1}^N$ trở thành một quá trình ngẫu nhiên liên tục $\lbrace \bold{x}(t) \rbrace_{t=0}^1$,
$\lbrace \sigma_i \rbrace_{i=1}^N$ trở thành một hàm $\sigma(t)$ và $\bold{z}_i$ trở thành $\bold{z}(t)$, ta sử dụng biến thời gian liên tục $t \in \lbrack 0, 1 \rbrack$ dành cho chỉ số thay vì số nguyên $i \in \lbrace 1, 2, \dots, N \rbrace$.
Đặt $\bold{x} \big ( \dfrac{i}{N} \big)=\bold{x}_i, \sigma \big( \dfrac{i}{N} \big) = \sigma_i$,
và $\bold{z} \big ( \dfrac{i}{N} \big)=\bold{z}_i$ với $i = 1, 2, \dots, N$.
Ta có thể viết lại công thức \ref{eq:20} với $\Delta t = \dfrac{1}{N}$ và $t \in \lbrace 0, \dfrac{1}{N}, \dots, \dfrac{N-1}{N} \rbrace$:

\begin{equation*}
    \bold{x}(t + \Delta t) = \bold{x}(t) + \sqrt{\sigma^2 (t + \Delta t) - \sigma^2 (t)} \bold{z}(t) \approx \bold{x} (t) + \sqrt{\dfrac{d \lbrack \sigma^2 (t) \rbrack}{dt}\Delta t} \bold{z}(t)
\end{equation*}

với đẳng thức xấp xỉ đúng với $\Delta t \ll 1$.
Trong giới hạn $\Delta t \rightarrow 0$, đẳng thức này tiến đến:

\begin{equation} \label{eq:21}
    d \bold{x} = \sqrt{\dfrac{d \lbrack \sigma^2(t) \rbrack}{dt}} d \bold{w}
\end{equation}

là phương trình VE SDE

Cho nhân xáo trộn $\lbrace p_{\alpha_i} (\bold{x} \vert \bold{x}_0) \rbrace_{i=1}^N$ được sử dụng trong DDPM, xích Markov rời rạc là:

\begin{equation} \label{eq:22}
    \bold{x}_i = \sqrt{1 - \beta_i} \bold{x}_{i-1} + \sqrt{\beta_i} \bold{z}_{i-1}, i = 1, \dots, N
\end{equation}

với $\bold{z}_{i-1} \sim \mathcal{N}(\bold{0}, \bold{I})$.
Để thu được giới hạn của xích Markov này khi $N \rightarrow \infty$, ta định nghĩa một tập phụ trợ các mức nhiễu $\lbrace \bar{\beta}_i = N \beta_i \rbrace_{i=1}^N$ và viết lại phương trình \ref{eq:22} như dưới đây:

\begin{equation} \label{eq:23}
    \bold{x}_i = \sqrt{1 - \dfrac{\bar{\beta}_i}{N}} \bold{x}_{i-1} + \sqrt{\dfrac{\bar{\beta}_i}{N}} \bold{z}_{i-1}, i = 1, \dots, N
\end{equation}

Trong giới hạn $N \rightarrow \infty, \lbrace \bar{\beta}_i \rbrace_{i=1}^N$ trở thành hàm $\beta(t)$ được đánh chỉ số bởi $t \in \lbrack 0, 1 \rbrack$.
Đặt $\beta \big ( \dfrac{i}{N} \big) = \bar{\beta}_i$,
$\bold{x}\big( \dfrac{i}{N} \big) = \bold{x}_i, \bold{z}\big( \dfrac{i}{N} \big) = \bold{z}_i$.
Ta có thể viết xích Markov ở công thức \ref{eq:23} thành như sau với $\Delta t = \dfrac{1}{N}$ và $t \in \lbrace 0, 1, \dots, \dfrac{N-1}{N} \rbrace$:

\begin{equation} \label{eq:24}
    \begin{aligned}
        \bold{x}(t + \Delta t) &= \sqrt{1 - \beta(t + \Delta t) \Delta t} \bold{x}(t) + \sqrt{\beta(t + \Delta t) \Delta t} \bold{z}(t) \\
        &\approx \bold{x}(t) - \dfrac{1}{2}\beta(t + \Delta t) \Delta t \bold{x} (t) + \sqrt{\beta(t + \Delta t) \Delta t} \bold{z} (t) \\
        &\approx \bold{x}(t) - \dfrac{1}{2} \beta(t) \Delta t \bold{x} (t) + \sqrt{\beta(t) \Delta t} \bold{z} (t)
    \end{aligned}
\end{equation}

với đẳng thức xấp xỉ đúng khi $\Delta t \ll 1$.
Vì vậy, trong giới hạn $\Delta t \rightarrow$, công thức \ref{eq:24} hội tụ về phương trình VP SDE có dạng sau:

\begin{equation} \label{eq:25}
    d \bold{x} = - \dfrac{1}{2} \beta(t) \bold{x} dt + \sqrt{\beta(t)} d \bold{w}
\end{equation}

Tới đây, ta đã chỉ ra được là nhiễu làm xáo trộn được sử dụng trong mô hình SMLD và DDPM tương ứng với rời rạc hóa của VE và VP SDEs.
Với VE SDE luôn luôn thu được một quá trình với phương sai bùng nổ khi $t \rightarrow \infty$.
Ngược lại, với VP SDE ta luôn thu được một quá trình với phương sai bị chặn.
Hơn nữa, quá trình có phương sai đơn vị và là hằng số với mọi $t \in \brack 0;\infty $ khi $p(\bold{x}(0))$ có phương sai đơn vị.
Khi VP SDE có độ trôi và các hệ số khuếch tán là affine, ta có thể sử dụng công thức 5.51 trong \citep{sarkka2019applied} để thu được phương trình ODE chi phối sự phá triển của phương sai:

\begin{equation*}
    \dfrac{d \Sigma_{VP}(t)}{dt} = \beta(t) (\bold{I} - \Sigma_{VP}(t))
\end{equation*}

với $\Sigma_{VP}(t) := \Cov \lbrack \bold{x}(t) \rbrack$ với $\lbrace \bold{x}(t) \rbrace_{t=0}^1$ thỏa mãn một phương trình VP SDE.
Giải phương trình ODE này, ta thu được:

\begin{equation}\label{eq:26}
    \Sigma_{VP}(t) = \bold{I} + e^{\int_{0}^t - \beta(s)ds} (\Sigma_{VP}(t) - \bold{I})
\end{equation}

Ta nhận thấy rất rõ ràng rằng $\Sigma_{VP}(t)$ luôn luôn bị chặn khi cho trước $\Sigma_{VP}(0)$.
Hơn nữa, $\Sigma_{VP}(t) \equiv \bold{I}$ nếu $\Sigma_{VP}(0) = \bold{I}$.
Bởi vì sự khác biệt này, ta đặt tên công thức \ref{eq:9} là Variance Exploding (VE) SDE và công thức \ref{eq:11} là Variance Preserving (VP) SDE/

\end{document}